#!/usr/bin/env python3
"""–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Silero VAD."""

import time
import numpy as np
import torch
from typing import Dict, Any, Tuple
from collections import deque


class ModernSpeechDetector:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Silero VAD."""
    
    def __init__(self, config: Dict[str, Any], sample_rate: int = 8000, chunk_size: int = 2400):
        self.config = config
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.silence_duration = config.get('silence_duration', 1.5)
        self.min_speech_duration = config.get('min_speech_duration', 0.8)
        self.listening_timeout = config.get('listening_timeout', 25)
        self.chunk_timeout = config.get('chunk_timeout', 0.05)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Silero VAD
        self.threshold = config.get('threshold', 0.5)
        self.min_silence_duration = config.get('min_silence_duration', 0.5)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        self.speech_detected = False
        self.speech_start_time = None
        self.silence_chunks = 0
        self.speech_chunks = 0
        self.last_speech_time = None
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏
        self.required_silence_chunks = int(self.silence_duration / self.chunk_timeout)
        self.min_speech_chunks = int(self.min_speech_duration / self.chunk_timeout)
        
        # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.audio_buffer = deque(maxlen=int(3.0 * sample_rate))
        self.speech_probabilities = deque(maxlen=20)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Silero VAD
        self._init_silero_vad()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.total_speech_time = 0.0
        self.total_silence_time = 0.0
        
        print(f"üîß –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ—á–∏ (Silero VAD):")
        print(f"   - –ê–Ω–∞–ª–∏–∑ 3-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –æ—Ç—Ä–µ–∑–∫–æ–≤ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≥–æ–ª–æ—Å–∞")
        print(f"   - –ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏: {self.min_speech_duration}s")
        print(f"   - –ü–æ—Ä–æ–≥ VAD: {self.threshold}")
        print(f"   - –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –≥–æ–ª–æ—Å–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–µ–∫—É–Ω–¥—ã")
    
    def _init_silero_vad(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Silero VAD –º–æ–¥–µ–ª–∏."""
        try:
            import torch
            from silero_vad import load_silero_vad
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Silero VAD —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            self.vad_model, self.utils = load_silero_vad(
                torch.jit.load, 
                'silero_vad.jit'
            )
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            self.vad_model.eval()
            self.vad_model = self.vad_model.to('cpu')
            
            print(f"‚úÖ Silero VAD –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Silero VAD: {e}")
            print(f"‚ö†Ô∏è –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏...")
            
            try:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏
                import torch
                from silero_vad import load_silero_vad
                
                self.vad_model, self.utils = load_silero_vad(
                    torch.jit.load, 
                    'silero_vad.jit'
                )
                
                self.vad_model.eval()
                self.vad_model = self.vad_model.to('cpu')
                
                print(f"‚úÖ Silero VAD –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (ONNX —Ä–µ–∂–∏–º)")
            except Exception as e2:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ Silero VAD: {e2}")
                print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–µ–∂–∏–º - –¥–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
                self.vad_model = None
                self.utils = None
    
    def _calculate_energy(self, audio_chunk: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ —á–∞–Ω–∫–∞."""
        if len(audio_chunk) == 0:
            return 0.0
        return np.sqrt(np.mean(audio_chunk.astype(np.float32) ** 2))
    
    def _calculate_spectral_centroid(self, audio_chunk: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥."""
        if len(audio_chunk) < 64:
            return 0.0
        
        try:
            fft = np.fft.fft(audio_chunk)
            magnitude = np.abs(fft[:len(fft)//2])
            
            if np.sum(magnitude) == 0:
                return 0.0
            
            freqs = np.linspace(0, self.sample_rate/2, len(magnitude))
            centroid = np.sum(freqs * magnitude) / np.sum(magnitude)
            return centroid
        except:
            return 0.0
    
    def _is_speech_silero_vad(self, audio_chunk: np.ndarray) -> Tuple[bool, float]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é Silero VAD."""
        if self.vad_model is None or len(audio_chunk) == 0:
            # –ï—Å–ª–∏ VAD –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω - —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —Ä–µ—á–∏ –Ω–µ—Ç
            print(f"üîá VAD –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π —á–∞–Ω–∫ - —Ä–µ—á—å: False")
            return False, 0.0
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ torch tensor
            audio_tensor = torch.from_numpy(audio_chunk.astype(np.float32))
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞—É–¥–∏–æ
            if len(audio_tensor) > 0:
                max_val = torch.max(torch.abs(audio_tensor))
                if max_val > 0:
                    audio_tensor = audio_tensor / max_val
                else:
                    print(f"üîá –ù—É–ª–µ–≤–∞—è –∞–º–ø–ª–∏—Ç—É–¥–∞ - —Ä–µ—á—å: False")
                    return False, 0.0
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ—á–∏
            with torch.no_grad():
                speech_prob = self.vad_model(audio_tensor, self.sample_rate).item()
            
            # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
            effective_threshold = self.threshold + 0.2
            is_speech = speech_prob > effective_threshold
            
            # –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            print(f"üéØ Silero VAD: prob={speech_prob:.3f}, threshold={effective_threshold:.3f}, speech={is_speech}")
            
            return is_speech, speech_prob
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Silero VAD: {e}")
            return False, 0.0
    
    def _is_speech_energy(self, audio_chunk: np.ndarray) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ—á—å –ø–æ —ç–Ω–µ—Ä–≥–∏–∏."""
        energy = self._calculate_energy(audio_chunk)
        self.energy_history.append(energy)
        
        if len(self.energy_history) > 5:
            avg_energy = np.mean(list(self.energy_history)[-5:])
            adaptive_threshold = max(self.energy_threshold, avg_energy * 0.6)
        else:
            adaptive_threshold = self.energy_threshold
        
        return energy > adaptive_threshold
    
    def _analyze_3_second_segment(self) -> bool:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å 3-—Å–µ–∫—É–Ω–¥–Ω—ã–π –æ—Ç—Ä–µ–∑–æ–∫ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä–µ—á–∏."""
        if len(self.audio_buffer) < self.sample_rate * 3:  # –ú–µ–Ω—å—à–µ 3 —Å–µ–∫—É–Ω–¥
            print(f"üîç –ê–Ω–∞–ª–∏–∑ 3—Å: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(self.audio_buffer)}/{self.sample_rate * 3})")
            return False
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–µ–∫—É–Ω–¥—ã –∏–∑ –±—É—Ñ–µ—Ä–∞
        segment = np.array(list(self.audio_buffer)[-self.sample_rate * 3:])
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ 0.5 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        chunk_size = self.sample_rate // 2  # 0.5 —Å–µ–∫—É–Ω–¥—ã
        speech_chunks = 0
        total_chunks = 0
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑ 3-—Å–µ–∫—É–Ω–¥–Ω–æ–≥–æ –æ—Ç—Ä–µ–∑–∫–∞: {len(segment)} —Å—ç–º–ø–ª–æ–≤, —á–∞–Ω–∫–∏ –ø–æ {chunk_size}")
        
        for i in range(0, len(segment) - chunk_size, chunk_size):
            chunk = segment[i:i + chunk_size]
            is_speech, confidence = self._is_speech_silero_vad(chunk)
            total_chunks += 1
            
            if is_speech:
                speech_chunks += 1
        
        # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 30% —á–∞–Ω–∫–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–µ—á—å - —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –µ—Å—Ç—å –≥–æ–ª–æ—Å
        speech_ratio = speech_chunks / max(total_chunks, 1)
        has_speech = speech_ratio > 0.3
        
        print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ 3—Å: {speech_chunks}/{total_chunks} —á–∞–Ω–∫–æ–≤ —Å —Ä–µ—á—å—é, ratio={speech_ratio:.2f}, has_speech={has_speech}")
        
        return has_speech
    
    def _is_speech_combined(self, audio_chunk: np.ndarray) -> Tuple[bool, float]:
        """–î–µ—Ç–µ–∫—Ü–∏—è —Ä–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Silero VAD –º–æ–¥–µ–ª–∏ –∏–ª–∏ fallback –Ω–∞ —ç–Ω–µ—Ä–≥–∏—é."""
        # –ü—Ä–æ–±—É–µ–º Silero VAD
        silero_result, confidence = self._is_speech_silero_vad(audio_chunk)
        
        # –ï—Å–ª–∏ VAD –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –ø–æ —ç–Ω–µ—Ä–≥–∏–∏
        if self.vad_model is None:
            energy = self._calculate_energy(audio_chunk)
            # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è fallback —Ä–µ–∂–∏–º–∞
            energy_threshold = 500.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –µ—â–µ –±–æ–ª—å—à–µ
            is_speech = energy > energy_threshold
            confidence = min(energy / 2000.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–Ω–µ—Ä–≥–∏—é
            print(f"üîß Fallback —Ä–µ–∂–∏–º: energy={energy:.1f}, threshold={energy_threshold:.1f}, speech={is_speech}")
            return is_speech, confidence
        
        # –ï—Å–ª–∏ Silero VAD –≥–æ–≤–æ—Ä–∏—Ç "—Ä–µ—á—å" - –≤–µ—Ä–∏–º –µ–º—É
        if silero_result:
            return True, confidence
        
        # –ï—Å–ª–∏ Silero VAD –Ω–µ —É–≤–µ—Ä–µ–Ω - —Å—á–∏—Ç–∞–µ–º —Ç–∏—à–∏–Ω–æ–π
        return False, confidence
    
    def _should_end_speech(self, current_time: float) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–ª–µ–¥—É–µ—Ç –ª–∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–µ—á—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ 3-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –æ—Ç—Ä–µ–∑–∫–æ–≤."""
        if not self.speech_detected:
            return False
        
        speech_duration = current_time - self.speech_start_time if self.speech_start_time else 0
        
        if speech_duration < self.min_speech_duration:
            return False
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º 3-—Å–µ–∫—É–Ω–¥–Ω—ã–π –æ—Ç—Ä–µ–∑–æ–∫
        # –ï—Å–ª–∏ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–µ–∫—É–Ω–¥—ã –Ω–µ—Ç –≥–æ–ª–æ—Å–∞ - –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–µ—á—å
        if len(self.audio_buffer) >= self.sample_rate * 3:
            has_speech_in_3_seconds = self._analyze_3_second_segment()
            if not has_speech_in_3_seconds:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–∏—à–∏–Ω–∞ –¥–ª–∏—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ª–≥–æ
                time_since_last_speech = current_time - self.last_speech_time if self.last_speech_time else 0
                if time_since_last_speech >= self.silence_duration:
                    print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–æ –∞–Ω–∞–ª–∏–∑—É 3-—Å–µ–∫—É–Ω–¥–Ω–æ–≥–æ –æ—Ç—Ä–µ–∑–∫–∞ (–Ω–µ—Ç –≥–æ–ª–æ—Å–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–µ–∫—É–Ω–¥—ã, —Ç–∏—à–∏–Ω–∞: {time_since_last_speech:.1f}s)")
                    return True
                else:
                    print(f"üîç 3-—Å–µ–∫—É–Ω–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª —Ç–∏—à–∏–Ω—É, –Ω–æ —Ç–∏—à–∏–Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è ({time_since_last_speech:.1f}s < {self.silence_duration}s)")
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏—è (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ)
        if speech_duration >= self.listening_timeout:
            print(f"‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {speech_duration:.1f}s, –ª–∏–º–∏—Ç: {self.listening_timeout}s)")
            return True
        
        return False
    
    def process_chunk(self, audio_chunk: np.ndarray) -> bool:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —á–∞–Ω–∫ –∞—É–¥–∏–æ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å –ª–∏ —Ä–µ—á—å."""
        if len(audio_chunk) == 0:
            return False
        
        current_time = time.time()
        self.audio_buffer.extend(audio_chunk)
        
        is_speech, confidence = self._is_speech_combined(audio_chunk)
        self.speech_probabilities.append(confidence)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"üìä –ß–∞–Ω–∫: speech={is_speech}, conf={confidence:.3f}, buffer={len(self.audio_buffer)}")
        
        if is_speech:
            if not self.speech_detected:
                self.speech_detected = True
                self.speech_start_time = current_time
                self.silence_chunks = 0
                self.speech_chunks = 0
                self.last_speech_time = current_time
                print(f"ÔøΩÔøΩ –†–µ—á—å –Ω–∞—á–∞–ª–∞—Å—å (SpeechBrain, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            else:
                self.speech_chunks += 1
                self.silence_chunks = 0
                self.last_speech_time = current_time
                self.total_speech_time += self.chunk_timeout
                
                if self.speech_chunks % 40 == 0:
                    speech_duration = current_time - self.speech_start_time
                    print(f"üé§ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–µ—á–∏ ({speech_duration:.1f}s, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
        else:
            if self.speech_detected:
                self.silence_chunks += 1
                self.total_silence_time += self.chunk_timeout
                
                if self.silence_chunks == 1:
                    print(f"üîá –ù–∞—á–∞–ª–æ —Ç–∏—à–∏–Ω—ã (SpeechBrain, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
                elif self.silence_chunks % 20 == 0:
                    speech_duration = current_time - self.speech_start_time if self.speech_start_time else 0
                    print(f"üîá –¢–∏—à–∏–Ω–∞ {self.silence_chunks}/{self.required_silence_chunks} (—Ä–µ—á—å: {speech_duration:.1f}s)")
                
                if self._should_end_speech(current_time):
                    speech_duration = current_time - self.speech_start_time if self.speech_start_time else 0
                    print(f"‚úÖ –†–µ—á—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—Ä–µ—á—å: {speech_duration:.1f}s, —Ç–∏—à–∏–Ω–∞: {self.silence_chunks} —á–∞–Ω–∫–æ–≤)")
                    return True
            else:
                self.silence_chunks += 1
                self.total_silence_time += self.chunk_timeout
                if self.silence_chunks % 100 == 0:
                    print(f"üîá –û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ—á–∏... ({self.silence_chunks * self.chunk_timeout:.1f}s)")
        
        if self.speech_detected and self.last_speech_time:
            time_since_speech = current_time - self.last_speech_time
            if time_since_speech > self.listening_timeout:
                print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è ({self.listening_timeout}s)")
                return True
        
        return False
    
    def reset(self):
        """–°–±—Ä–æ—Å–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞."""
        self.speech_detected = False
        self.speech_start_time = None
        self.silence_chunks = 0
        self.speech_chunks = 0
        self.last_speech_time = None
        self.audio_buffer.clear()
        self.speech_probabilities.clear()
        self.total_speech_time = 0.0
        self.total_silence_time = 0.0
        print("üîÑ –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å–±—Ä–æ—à–µ–Ω")
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞."""
        return {
            'speech_detected': self.speech_detected,
            'speech_duration': time.time() - self.speech_start_time if self.speech_start_time else 0,
            'silence_chunks': self.silence_chunks,
            'speech_chunks': self.speech_chunks,
            'threshold': self.threshold,
            'speech_ratio': self.total_speech_time / max(self.total_speech_time + self.total_silence_time, 1e-6),
            'avg_speech_prob': np.mean(list(self.speech_probabilities)) if self.speech_probabilities else 0.0
        }

# Vosk-TTS сервис с Triton Inference Server
version: '3.8'

services:
  triton-tts:
    image: nvcr.io/nvidia/tritonserver:23.04-py3
    command: |
      bash -c "
        pip install vosk-tts onnxruntime soundfile librosa numpy &&
        tritonserver --model-repository=/models --allow-http --allow-grpc --log-verbose=1
      "
    ports:
      - "18100:8000"  # HTTP (отличается от ASR - 8001)
      - "18101:8001"  # GRPC  
      - "18102:8002"  # Metrics
    volumes:
      - ./triton_model_repository:/models
      - ./src:/workspace/src
    working_dir: /workspace
    networks:
      - tts-net
    restart: unless-stopped
    environment:
      - CUDA_VISIBLE_DEVICES=0  # Для GPU поддержки
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
    # Раскомментировать для GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # TTS API сервер (обертка над Triton)
  vosk-tts-api:
    build: .
    ports:
      - "18200:8080"  # TTS API
    environment:
      - TTS__TRITON_URL=triton-tts:8000
      - LOG_LEVEL=INFO
      - TTS_CLI_ONLY=0
    depends_on:
      - triton-tts
    volumes:
      - ./output:/app/output
      - ./logs:/app/logs
    networks:
      - tts-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus для мониторинга
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9091:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - tts-net
    restart: unless-stopped

networks:
  tts-net:
    driver: bridge

volumes:
  tts-models:
  tts-output:
  tts-logs:
